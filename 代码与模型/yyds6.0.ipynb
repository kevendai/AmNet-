{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d629914-fc8a-449e-8811-ccb7deab39de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from GetdataSet import trainMYDataSet\n",
    "from LOSS import PerceptualLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "from os.path import join\n",
    "import gc\n",
    "\n",
    "train_batch_size = 5\n",
    "start_epochs = 0\n",
    "learning_rate = 0.01*(0.5**(start_epochs//30))\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daec9cb2-1b14-499f-b261-11e408be3c8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ShareSepConv(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(ShareSepConv, self).__init__()\n",
    "        assert kernel_size % 2 == 1, 'kernel size should be odd'\n",
    "        self.padding = (kernel_size - 1) // 2  # 设置该大小的padding,能使得进行卷积后，输出的特征图的尺寸大小不变\n",
    "        weight_tensor = torch.zeros(1, 1, kernel_size, kernel_size)  # 定义一个1个种类,一个通道，大小为kernel_size的卷积核\n",
    "        weight_tensor[0, 0, (kernel_size - 1) // 2, (kernel_size - 1) // 2] = 1  # 将卷积核中间那个数值设为1\n",
    "        self.weight = nn.Parameter(weight_tensor)  # 将其卷积核变为可学习的参数\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        inc = x.size(1)  # 获取输入特征图的通道数\n",
    "        expand_weight = self.weight.expand(inc, 1, self.kernel_size, self.kernel_size).contiguous()\n",
    "        return F.conv2d(x, expand_weight,\n",
    "                        None, 1, self.padding, 1, inc)\n",
    "\n",
    "\n",
    "class SmoothDilatedResidualBlock(nn.Module):\n",
    "    def __init__(self, channel_num, dilation=1, group=1):\n",
    "        super(SmoothDilatedResidualBlock, self).__init__()\n",
    "        self.pre_conv1 = ShareSepConv(dilation * 2 - 1)\n",
    "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group,\n",
    "                               bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
    "        self.pre_conv2 = ShareSepConv(dilation * 2 - 1)\n",
    "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group,\n",
    "                               bias=False)\n",
    "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.norm1(self.conv1(self.pre_conv1(x))))\n",
    "        y = self.norm2(self.conv2(self.pre_conv2(y)))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channel_num, dilation=1, group=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group,\n",
    "                               bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
    "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group,\n",
    "                               bias=False)\n",
    "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.norm1(self.conv1(x)))\n",
    "        y = self.norm2(self.conv2(y))\n",
    "        return F.relu(x + y)\n",
    "\n",
    "\n",
    "class CINR(nn.Module):\n",
    "    def __init__(self, in_channel_num, out_channel_num, kernel_size=3, stride=1, padding=1):\n",
    "        super(CINR, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel_num, out_channel_num, kernel_size, stride, padding, bias=False)\n",
    "        self.norm1 = nn.InstanceNorm2d(out_channel_num, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        return F.relu(x)\n",
    "\n",
    "\n",
    "# 通道注意力机制\n",
    "class Attu_1(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(Attu_1, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(channel_num, channel_num, 1, 1, 0, bias=False)\n",
    "        self.conv2 = nn.Conv2d(channel_num, channel_num, 1, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(self.avg_pool(x)))\n",
    "        y = torch.sigmoid(self.conv2(y))\n",
    "        return x * y\n",
    "\n",
    "\n",
    "# 残差组，包含三个残差块\n",
    "class ResidualGroup(nn.Module):\n",
    "    def __init__(self, channel_num, dilation=1, group=1):\n",
    "        super(ResidualGroup, self).__init__()\n",
    "        self.residual_block1 = ResidualBlock(channel_num, dilation, group)\n",
    "        self.residual_block2 = ResidualBlock(channel_num, dilation, group)\n",
    "        self.residual_block3 = ResidualBlock(channel_num, dilation, group)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.residual_block1(x)\n",
    "        y = self.residual_block2(y)\n",
    "        y = self.residual_block3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# 像素注意力机制\n",
    "class Attu_2(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(Attu_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_num, channel_num, 1, 1, 0, bias=False)\n",
    "        self.conv2 = nn.Conv2d(channel_num, channel_num, 1, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = torch.sigmoid(self.conv2(y))\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class AmNet(nn.Module):\n",
    "    def __init__(self, in_c=3, out_c=3):\n",
    "        super(AmNet, self).__init__()\n",
    "        self.Cinr1 = CINR(in_c, 64, 3, 1, 1)\n",
    "\n",
    "        # MFE\n",
    "        self.Mfe1_1 = nn.Conv2d(64, 64, 3, 1, 1, bias=False)\n",
    "        self.Mfe1_2 = CINR(64, 64, 3, 1, 1)\n",
    "        self.Mfe1_3 = Attu_1(64)\n",
    "\n",
    "        self.Mfe2_1 = SmoothDilatedResidualBlock(64, dilation=2)\n",
    "        self.Mfe2_2 = CINR(64, 64, 3, 1, 1)\n",
    "        self.Mfe2_3 = Attu_1(64)\n",
    "\n",
    "        self.Mfe3_1 = SmoothDilatedResidualBlock(64, dilation=4)\n",
    "        self.Mfe3_2 = CINR(64, 64, 3, 1, 1)\n",
    "        self.Mfe3_3 = Attu_1(64)\n",
    "\n",
    "        self.Mfe_final = nn.Conv2d(64 * 3, 64, 3, 1, 1, bias=False)\n",
    "\n",
    "        self.Cinr2_1 = CINR(64, 64, 3, 1, 1)\n",
    "        self.max_pool2_1 = nn.MaxPool2d(2, 2, 0)\n",
    "        self.Cinr2_2 = CINR(64, 128, 3, 1, 1)\n",
    "        self.max_pool2_2 = nn.MaxPool2d(2, 2, 0)\n",
    "        self.Cinr2_3 = CINR(128, 256, 3, 1, 1)\n",
    "\n",
    "        self.ResidualGroup3_1 = ResidualGroup(256, dilation=1)\n",
    "\n",
    "        self.Cinr3_1 = CINR(256, 256, 3, 1, 1)\n",
    "        self.deconv3_1 = nn.ConvTranspose2d(256 * 2, 128, 4, 2, 1, bias=False)\n",
    "        self.Cinr3_2 = CINR(128, 128, 3, 1, 1)\n",
    "        self.deconv3_2 = nn.ConvTranspose2d(128 * 2, 64, 4, 2, 1, bias=False)\n",
    "        self.Cinr3_3 = CINR(64, 64, 3, 1, 1)\n",
    "\n",
    "        self.Cinr4 = CINR(64 * 2, 64, 3, 1, 1)\n",
    "\n",
    "        self.Attu_Block1 = Attu_1(64)\n",
    "        self.Attu_Block2 = Attu_2(64)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(64, out_c, 3, 1, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.Cinr1(x)\n",
    "\n",
    "        y2 = self.Mfe1_3(self.Mfe1_2(self.Mfe1_1(y1)))\n",
    "        y3 = self.Mfe2_3(self.Mfe2_2(self.Mfe2_1(y1)))\n",
    "        y4 = self.Mfe3_3(self.Mfe3_2(self.Mfe3_1(y1)))\n",
    "        # concat连接特征\n",
    "        y = self.Mfe_final(torch.cat((y2, y3, y4), 1))  # 没确定特征连接方式感觉是cat\n",
    "        y6 = self.Cinr2_1(y + y1)  # 基础网络\n",
    "        y7 = self.Cinr2_2(self.max_pool2_1(y6))\n",
    "        y8 = self.Cinr2_3(self.max_pool2_2(y7))\n",
    "        y = self.Cinr3_1(self.ResidualGroup3_1(y8))\n",
    "        y = self.deconv3_1(torch.cat((y8, y), 1))\n",
    "        y = self.deconv3_2(torch.cat((self.Cinr3_2(y), y7), 1))\n",
    "        y = torch.cat((self.Cinr3_3(y), y6), 1)\n",
    "        y = self.Attu_Block2(self.Attu_Block1(self.Cinr4(y)))\n",
    "        y = self.conv5(y)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a741b28-1fec-4bbd-919e-99ecfb6e2448",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/ma-user/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b634d1e6c04b9998858d1f8d9cbe38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "smooth_l1 = nn.SmoothL1Loss()\n",
    "vgg16_Loss = PerceptualLoss()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AmNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defc0ba3-91d9-4886-986e-f8e9515e5f68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataPrefetcher():\n",
    "\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.batch = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.batch = None\n",
    "            return\n",
    "\n",
    "    def next(self):\n",
    "        batch = self.batch\n",
    "        self.preload()\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e00bebaf-a0d8-4c00-98ab-5ec131c485b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, start_epoch):\n",
    "    print(\"start train\")\n",
    "    model.train()\n",
    "    #steps_per_epoch = len(datasetloader)\n",
    "    #total_iteration = steps_per_epoch * num_epochs\n",
    "    #print(\"total_iteration:\", total_iteration)\n",
    "    total_loss = 0\n",
    "    for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "        dataset = trainMYDataSet(src_data_path=\"./input_train/\", lable_data_path=\"./gt_train/\")\n",
    "        datasetloader = DataLoader(dataset, batch_size=train_batch_size, shuffle=False, num_workers=0)\n",
    "        gc.collect()\n",
    "        start_time = time.time()\n",
    "        prefetcher = DataPrefetcher(datasetloader)\n",
    "        batch = prefetcher.next()\n",
    "        i = 0\n",
    "        epoch_loss = 0\n",
    "        while batch is not None:\n",
    "            i += 1\n",
    "            inputs = batch[0].to(device)\n",
    "            labels = batch[1].to(device)\n",
    "            R = model(inputs)\n",
    "            loss1 = smooth_l1(R, labels)\n",
    "            loss2 = vgg16_Loss(R, labels)\n",
    "            train_loss = loss1 + loss2\n",
    "            epoch_loss+=train_loss\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            batch = prefetcher.next()\n",
    "            with open(\"epoch_output.txt\", \"a\") as f:\n",
    "                if i % 5 == 0:\n",
    "                    f.write('{:.2f} => Epoch[{}/{}]: train_loss: {:.4f},l1: {:.4f},vgg: {:.4f}\\n'.format(time.time() - start_time, epoch, num_epochs,\n",
    "                                                                                      train_loss.item(),\n",
    "                                                                                      loss1.item(),\n",
    "                                                                                      loss2.item()))\n",
    "        if epoch % 5 == 0:\n",
    "            state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch}\n",
    "            backup_model_dir = join('./Rlablemodel/')\n",
    "            torch.save(state, join(backup_model_dir, '{}-model-epochs{}.pth'.format('AmNet', epoch)))\n",
    "        if epoch % 30 == 0 and epoch != 0:\n",
    "            for p in optimizer.param_groups:\n",
    "                p['lr'] *= 0.5\n",
    "        time_epoch = time.time() - start_time\n",
    "        epoch_loss = epoch_loss*1.0/i\n",
    "        total_loss += epoch_loss\n",
    "        print(\"==>No: {} epoch, time: {:.2f}, loss: {:.5f}\".format(epoch, time_epoch / 60, epoch_loss))\n",
    "        with open(\"output.txt\", \"a\") as f:\n",
    "            f.write(\"==>No: {} epoch, time: {:.2f}, loss: {:.5f}\\n\".format(epoch, time_epoch / 60, epoch_loss))\n",
    "    print(\"total_loss:\",total_loss*1.0/num_epochs-start_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000c7c0-d01a-4f8a-9e19-05f346aafdd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.01\n",
      "模型加载\n",
      "start train\n",
      "==>No: 21 epoch, time: 5.35, loss: 0.05980\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_epoch = start_epochs\n",
    "    print(torch.cuda.is_available())\n",
    "    print(learning_rate)\n",
    "    if start_epoch == 0:\n",
    "        print('==> 无保存模型，将从头开始训练！')\n",
    "    else:\n",
    "        print('模型加载')\n",
    "        checkpoint_model = join('./Rlablemodel/',\n",
    "                                '{}-model-epochs{}.pth'.format(\"AmNet\", start_epoch))\n",
    "        checkpoint = torch.load(checkpoint_model, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    train(model, start_epoch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}